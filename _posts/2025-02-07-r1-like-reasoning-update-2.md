---
title: Lessons on Reproducing R1-like Reasoning in Small LLMs without using DeepSeek-R1-Zero (or its derivatives) - Update 2
date: 2025-02-07
---

Written by Akash Srivastava, Isha Puri, Kai Xu, Shivchander Sudalairaj, Mustafa Eyceoz, Oleg Silkin, Abhishek Bhandwaldar, Aldo Genaro Pareja Cardona, GX Xu

> This is the second update on our journey to reproduce R1-like reasoning in small LLMs.
> The original blog post can be found [here](https://red-hat-ai-innovation-team.github.io/posts/r1-like-reasoning) and the first update is [here](https://red-hat-ai-innovation-team.github.io/posts/r1-like-reasoning-update-1).

---

**Todayâ€™s Updates: More Experiments, More Insights**

Yesterday, we ran **two new experiments** to push our small models even further:

ğŸ”¹ **Testing the LIMO dataset on Granite** â€“ Can a really small model develop reasoning abilities with just **\~800 high-quality examples**?

Unfortunately, this one **didnâ€™t pan out**. Neither **Llama** nor **Granite** showed much improvement, even though this dataset significantly boosted **Phi-4â€™s** performance. The original paper demonstrated strong results on **Qwen-32B**, but based on our experiment, itâ€™s clear that the effectiveness of this approach is **very model-dependent**.

In short: **Qwen-32B is just a beast.** It already has strong mathematical and reasoning abilities, so training on a relatively tiny dataset helps refine whatâ€™s already there. For smaller models? Not so much. (*Guess thereâ€™s no such thing as a free lunch after all\! ğŸ˜…*)

ğŸ”¹ **Generating synthetic data using particle filtering on LIMO dataset questions** â€“ Could this enhance reasoning abilities?

This one was interesting\! Running **Phi-4** with our **particle filtering-based inference scaling method**, it successfully solved **about half** of the **\~800 LIMO problems** using a **512-particle count**.

Hereâ€™s what happened next:

* We **built a backtracking-based reasoning dataset** using these filtered solutions and fine-tuned the same **Phi-4** model that we used for generation.

* **Did it work?** Nope. The model actually solved **fewer AIME24 problems** than the base model. âŒ

* However, when we trained using **only the correct solution dataset**, the model managed to **preserve its performance**.

* Comparing the **LIMO dataset solutions** with those from Phi-4, we found that **LIMO solutions were 2â€“3 times longer**.

* Training on a **380-sample subset** of this data **slightly improved AIME24 performance**, but *only* by solving **one more question**. ğŸ¤·â€â™‚ï¸

**Whatâ€™s Next? New Experiments Underway ğŸš€**

We finally **killed off our older GRPO runs** after running them for quite a few iterations. The reason? **The reward had plateaued**, and the trained models showed **no further improvements** on AIME24.

At this point, Iâ€™m starting to wonder: **Is AIME24 just too difficult for small models unless theyâ€™ve been trained with distilled data from larger reasoning models?** ğŸ¤” Weâ€™ll keep using it for now, but we might reconsider another benchmark later.

Today, we launched **two new experiments**:

ğŸ”¹ **GRPO on â€œBut Waitâ€ Phi Checkpoint & LIMO Questions**

* Weâ€™re testing if the **increased difficulty** of the **LIMO** questions can **trigger any reasoning sparks** in our best **â€œBut Waitâ€ Phi checkpoint**â€”which already shows **R1-style reflection and reasoning**.

ğŸ”¹ **Introducing GRPO-Direct**

* Instead of our usual **â€œgenerate synthetic data â†’ SFT â†’ GRPOâ€** loop, weâ€™re trying a **direct** approach:

	1\.	**Generate synthetic data** inside **GRPO** itself.

	2\.	**Immediately train the model on it** within the same loop.

Weâ€™re running this on the **LIMO dataset**, using a **Phi checkpoint** that has already been trained on synthetic data it generated from the **380 LIMO samples**.

---

If you want to cite our work, you can use the following BibTeX entry of the original blog post.

```bibtex
@misc{srivastava2024lessonsonreproducing,  
      title={Lessons on Reproducing R1-like Reasoning in Small LLMs without using DeepSeek-R1-Zero (or its derivatives)},  
      author={Akash Srivastava, Isha Puri, Kai Xu, Shivchander Sudalairaj, Mustafa Eyceoz, Oleg Silkin, Abhishek Bhandwaldar, Aldo Genaro Pareja Cardona and GX Xu},  
      url={https://red-hat-ai-innovation-team.github.io/posts/r1-like-reasoning},  
      year={2025},  
}  
```